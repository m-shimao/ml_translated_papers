{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 気づき\n",
    "\n",
    "- アブストだけ読んでもわからん。。。出て来る用語とか論文の文脈とかわからんので。結局いまの知識レベルだと、内容読まないとわかんない。\n",
    "- 思ったよりGoogle翻訳でいける（数式が少ない論文だから？）。ただ、変なところに改行が入ると途端にGoogle翻訳が全然ダメになるので気をつけないといけない。\n",
    "- 関連する論文が山のように出てきて、結果それらを調べる余裕が無いため、記述をスルーしてしまっている…。\n",
    "- 課題論文の選択、結構重要かも。ベーシックなものにしたほうがよいかも。（関連情報が多すぎる）\n",
    "- `サイズが512×512の画像を20FPSで処理し、リアルタイムまたはビデオで画風変換を実行することが可能` これはすげぇ…！\n",
    "- 目的を踏まえつつやり方考えたほうが良いかも。ただ、論文を和訳して出来る限りの内容を理解する、って形が最適なのかな、と。\n",
    "    - トレンドを追う気持ちを忘れない\n",
    "        - 論文を読むことに対する抵抗を下げる\n",
    "        - 一方でトレンド情報は必ずしも論文である必要はない気がする（Qiitaとかブログとか）\n",
    "    - 英語に日常的に触れる\n",
    "\n",
    "## 参考サイト\n",
    "\n",
    "- 本論文\n",
    "https://qiita.com/dsanno/items/a79a87720239f295234b\n",
    "\n",
    "- segmentation\n",
    "http://ai-4-u.com/tech/what-is-semantic-segmentation\n",
    "\n",
    "- FCN\n",
    "http://www.mathgram.xyz/entry/keras/fcn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20180214 論文和訳\n",
    "\n",
    "## 抄録\n",
    "\n",
    "リアルタイムな画風変換と超解像度のための知覚損失\n",
    "\n",
    "ジャスティン・ジョンソン、 アレクサンドルAlahi、 リチウムフェイ・フェイ\n",
    "（2016年3月27日に提出）\n",
    "\n",
    "我々は、入力画像が出力画像に変換される画像変換問題を考える。\n",
    "このような問題の最近の方法は、通常、出力画像とチェック画像の間にピクセルごとの損失を使用してフィードフォワードCNNが使われる。\n",
    "\n",
    "並行作業は、事前に訓練されたネットワークから抽出された高レベルの特徴に基づいて知覚損失関数を定義し、\n",
    "最適化することによって高品質の画像を生成することができることを示している。\n",
    "\n",
    "我々は、両方のアプローチの利点を組み合わせ、画像変換タスクのためのフィードフォワードネットワークをトレーニングするための知覚損失関数の使用を提案する。\n",
    "\n",
    "Gatysらによって提案された最適化問題をリアルタイムで解決するために、フィードフォワードネットワークが訓練されている画風変換に関する結果を示す。\n",
    "\n",
    "最適化に基づく方法と比較して、私たちのネットワークは同様の定性的結果をもたらしますが、3桁も高速です。\n",
    "また、ピクセル単位の損失を知覚的な損失で置き換えることで、視覚的に満足のいく結果が得られる、単一画像の超解像度を試しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "多くの古典的な問題は、画像変換タスクとしてフレーム化することができます。\n",
    "システムは、いくつかの入力画像を受け取り、それを出力画像に変換する。\n",
    "例：ノイズ除去、超解像、およびカラー化処理\n",
    "\n",
    "コンピュータビジョン（ロボットの目）の例としては、セマンティックセグメンテーション（1picxel毎の画像検出方法）\n",
    "および深度推定（画像からカメラと物体との距離を推定する）であり、\n",
    "入力はカラー画像であり、出力画像はシーンに関する意味的情報または幾何学的情報をencodeする。\n",
    "\n",
    "画像変換タスクを解決するための1つのアプローチは、\n",
    "ピクセルごとに出力画像とチェック画像との間の差を測定する損失関数を使ってフィードフォワードCNNを学習させることです。\n",
    "\n",
    "このアプローチは、例えば、Dongらが超解像度[1]、チェン（Cheng）らによるカラー化[2]、\n",
    "ロング（Long）らによるセグメンテーション[3]、および\n",
    "Eigenらは深さと表面法線（物体の向き）の予測[4,5]を行っている。\n",
    "\n",
    "そのようなアプローチは、テスト時に効率的であり、訓練されたネットワークを順方向に通過するだけでよい。\n",
    "しかしながら、これらの方法によって使用されるピクセルごとの損失は、出力画像とチェック画像との知覚的な差異を捉えていない。\n",
    "例えば、2つの同一の画像が1つの画素だけ互いにオフセットされているとする。 彼らの知覚にもかかわらず\n",
    "これらの類似性は、ピクセルごとの損失によって測定されたものは非常に異なるであろう。\n",
    "\n",
    "図1.画風変換（上）および×4超解像度（下）の結果の例\n",
    "画風変換に対して、Gatysら[10]と同様の結果を達成するが、より速い。\n",
    "超解像に対して、我々の知覚的損失を訓練した方法は、ピクセルごとの損失で訓練された方法と比較して細かい詳細をより良好に再構築することができる。\n",
    "\n",
    "並行して、最近の研究は、\n",
    "ピクセル間の差異ではなく予めトレーニングされた畳み込みニューラルネットワークから抽出された高水準画像特徴表現間の差異に基づく\n",
    "知覚損失関数を使うことで高品質の画像を生成できることを示している。\n",
    "\n",
    "画像は、損失関数を最小化することによって生成される。 \n",
    "この戦略は、\n",
    "Mahendranらによる特徴反転[6]、\n",
    "Simonyanら[7]およびYosinskiら[8]による特徴の視覚化、\n",
    "Gatysらによるテクスチャ合成と画風変換[9,10]に見られる。\n",
    "\n",
    "これらの手法は、高品質の画像を生成するが、推論は最適化問題を解決する必要があるため、速度が遅い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このホワイトペーパーでは、これらの2つのアプローチの利点を組み合わせています。\n",
    "我々は、画像変換タスクのためのフィードフォワード変換ネットワークを訓練するが、\n",
    "低レベルのピクセル情報のみに依存するピクセル毎の損失関数を使用するのではなく、\n",
    "事前に訓練された損失ネットワークからの高レベルの特徴に依存する知覚損失関数を使用することで我々のネットワークを訓練します。\n",
    "\n",
    "訓練中、知覚損失は画像類似性をピクセル単位の損失よりも堅牢に測定し、テスト時には変換ネットワークをリアルタイムで実行します。\n",
    "\n",
    "我々は、画風変換と単一画像超解像という2つのタスクを実験する。\n",
    "\n",
    "両方とも本質的に悪い考えです。\n",
    "スタイルの転送には正しい出力が1つもなく、超解像度には同じ低解像度入力を生成した可能性のある高解像度の画像が多数あります。\n",
    "どちらのタスクでも成功するには、入力イメージに関する意味論的推論が必要です。\n",
    "\n",
    "画風変換の場合、色とテクスチャの大幅な変更にもかかわらず、出力は入力と意味的に似ていなければなりません。\n",
    "超解像のためには、視覚的にあいまいな低解像度入力から細かい詳細を推測する必要があります。\n",
    "\n",
    "原則的に、いずれかのタスクのために訓練された大容量ニューラルネットワークは、暗黙のうちに関連するセマンティクスについて推論することを学ぶことができる。\n",
    "実際には、ゼロから学ぶ必要はありません。知覚損失関数の使用は、損失ネットワークから変換ネットワークへのセマンティック知識の転送を可能にします。\n",
    "\n",
    "画風変換のために、我々のフィードフォワードネットワークは[10]から最適化問題を解くように訓練されている。\n",
    "我々の結果は質的にも目的関数値でも[10]と似ているが、生成するのに3桁の速さである。\n",
    "\n",
    "超解像では、ピクセルごとの損失を知覚損失で置き換えると、×4および×8の超解像で視覚的に満足できる結果が得られることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Related Work\n",
    "\n",
    "### フィードフォワード画像変換。 \n",
    "\n",
    "近年、ピクセル毎の損失関数を用いて深い畳み込みニューラルネットワークを訓練することにより、\n",
    "多種多様なフィードフォワード画像変換タスクが解決されている。\n",
    "\n",
    "セマンティックセグメンテーション方法[3,5,12,13,14,15]は、\n",
    "入力画像上で完全畳み込み方式でネットワークを実行し、\n",
    "ピクセルごとの分類損失を伴う訓練によって高密度シーンラベルを生成する。\n",
    "\n",
    "[15]は、ネットワークの他の部分と共同して訓練された反復層として、\n",
    "CRF(条件付き確率場)推論をフレーミングすることによってピクセルごとの損失を超えて移動する。\n",
    "\n",
    "変換ネットワークのアーキテクチャは、\n",
    "ネットワーク内ダウンサンプリングを使用してフィーチャマップの空間的範囲を縮小し、\n",
    "ネットワーク内のアップサンプリングによって最終的な出力画像を生成する[3]および[14]に触発されている。\n",
    "\n",
    "最近の深さ[5,4,16]と表面法線推定[5,17]の方法は、\n",
    "[4] 、5]ピクセルごとの回帰またはクラス分類[17]の損失用いて訓練したフィードフォワード畳み込みネットワークを用いて\n",
    "カラー入力画像を幾何学的に意味のある出力画像に変換する点で類似している。\n",
    "\n",
    "いくつかの方法は、出力画像に局所的な一貫性を強制するために、\n",
    "画像勾配[5]にペナルティをかけることまたはCRF損失層[16]を使用することによって画素ごとの損失を超えるものに移行する。\n",
    "\n",
    "[2]では、ピクセルごとの損失を使用してグレースケール画像をカラーに変換するフィードフォワードモデルが訓練されています。\n",
    "\n",
    "### 知覚最適化。\n",
    "\n",
    "最近の論文の多くは、畳み込みネットワークから抽出された高レベルの特徴に応じて、\n",
    "知覚的である画像を生成するために最適化を使用している。\n",
    "\n",
    "訓練されたネットワークで符号化された機能を理解するために、\n",
    "クラス予測スコア[7,8]または個々の特徴[8]を最大にする画像を生成することができる。\n",
    "同様の最適化手法を用いて、信頼性の高いフーリング画像(NNを騙す画像)を生成することもできる[18,19]。\n",
    "\n",
    "MahendranとVedaldi [6]は、異なるネットワーク層によって保持される画像情報を理解するために、\n",
    "特徴再構成損失を最小化することによって畳み込みネットワークから特徴を反転させる。\n",
    "局所バイナリ記述子[20]とHOGフィーチャ[21]を逆にするためにこれまで同様の方法が使用されていました。\n",
    "\n",
    "DosovitskiyとBrox [22]の研究は、\n",
    "畳み込み特徴を逆転させるためにフィードフォワードニューラルネットワークを訓練し、\n",
    "[6]によってもたらされる最適化問題の解を素早く近似するので我々のものに特に関連している。\n",
    "\n",
    "しかし、それらのフィードフォワードネットワークはピクセルごとの再構築損失で訓練され、\n",
    "我々のネットワークは[6]の特徴再構成損失を直接的に最適化する。\n",
    "\n",
    "### 画風変換\n",
    "\n",
    "Gatysら[10]は、\n",
    "[6]の特徴再構成損失と事前畳み込みネットワークから抽出された特徴に基づく\n",
    "スタイル再構成損失を共同で最小化することにより、\n",
    "1つの画像の内容を別の画像のスタイルと組み合わせる芸術的画風変換を実行する。\n",
    "類似の方法が以前にテクスチャ合成に使用されていた[9]。\n",
    "\n",
    "それらの方法は、高品質の結果をもたらすが、\n",
    "最適化問題の各ステップは、前もって訓練されたネットワークを通る順方向および逆方向のパスを必要とするので、計算上高価である。\n",
    "\n",
    "この計算負荷を克服するために、フィードフォワードネットワークを訓練して最適化問題の解を素早く近似します。\n",
    "\n",
    "### 画像の超解像度\n",
    "\n",
    "画像の超解像度は、様々な技術が開発された古典的な問題である。\n",
    "\n",
    "Yangら[23]は、畳み込みニューラルネットワークの普及に先立ち、一般的な技術を網羅的に評価している。\n",
    "\n",
    "超解像技術を予測ベースの手法（バイリニア、バイキュービック、ランチョス[24]）、\n",
    "エッジベースの手法[25,26]、\n",
    "統計的手法[27,28,29]、\n",
    "パッチベースの手法[25,30、 31,32,33,34,35,36]、\n",
    "スパース辞書法[37,38]\n",
    "\n",
    "最近、[1]は、ピクセルごとのユークリッド損失を訓練した3層畳み込みニューラルネットワークを用いて、\n",
    "単一画像の超解像度において優れた性能を達成した。\n",
    "他の最近の最先端の方法には、[39,40,41]が含まれる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図2.システム概要。\n",
    "\n",
    "我々は、入力画像を出力画像に変換する画像変換ネットワークを訓練する。 我々は、画像分類のために事前に設定された損失ネットワークを使用して、画像間のコンテンツおよびスタイルの知覚差を測定する知覚損失関数を定義する。\n",
    "損失ネットワークは、訓練プロセス中に固定されたままである。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図2に示すように、\n",
    "我々のシステムは、画像変換ネットワークfWと、\n",
    "いくつかの損失関数 `l1...lk` を定義するために使用される損失ネットワークφの2つのコンポーネントで構成されています。\n",
    "\n",
    "画像変換ネットワークは、重みWによってパラメータ化された深い残差畳み込みニューラルネットワークである。 \n",
    "写像y = fW（x）を介して入力画像xを出力画像yに変換する。\n",
    "各損失関数は、出力画像yと目標画像yiとの間の差を測定するスカラー値i（y、yi）を計算する。\n",
    "\n",
    "画像変換ネットワークは、確率的勾配降下を用いて訓練され、損失関数の重みの組み合わせを最小にする。\n",
    "\n",
    "ピクセル単位の損失の欠点に対処し、\n",
    "損失関数が画像間の知覚的および意味的な差をよりよく測定できるようにするために、\n",
    "我々は最適化によって画像を生成する最近の研究からインスピレーションを引き出す[6,7,8,9,10]。\n",
    "\n",
    "これらの方法の重要な洞察は、\n",
    "画像分類のために事前に訓練された畳み込みニューラルネットワークが、\n",
    "我々が損失関数で測定したい知覚的および意味論的情報を符号化することを既に学んでいることである。\n",
    "\n",
    "したがって、我々は損失関数を定義するために固定損失ネットワークとして画像分類のために予め訓練されたネットワークφを利用する。\n",
    "我々の深い畳み込み変換ネットワークは、深い畳み込みネットワークである損失関数を用いて訓練される。\n",
    "\n",
    "損失ネットワークφは、画像間の内容およびスタイルの違いを測定する、特徴再構成損失およびスタイル再構成損失スタイルを定義するために使用される。\n",
    "\n",
    "各入力画像xについて、コンテンツターゲットycおよびスタイルターゲットysを有する。\n",
    "画風変換の場合、コンテンツターゲットycは入力画像xであり、\n",
    "出力画像yはx = ycのコンテンツとysのスタイルを結合する必要があります。\n",
    "私たちはスタイルターゲットごとに1つのネットワークを訓練します。\n",
    "\n",
    "単一画像超解像度の場合、入力画像xは低解像度入力であり、\n",
    "コンテンツターゲットycはチェックすべき高解像度画像であり、スタイル再構成損失は使用されない。\n",
    "超解像係数ごとに1つのネットワークを訓練します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.1 Image Transformation Networks\n",
    "\n",
    "我々の画像変換ネットワークは、Radfordら[42]が述べたアーキテクチャのガイドラインにほぼ従います。\n",
    "プールレイヤーを使用しません、代わりに\n",
    "ネットワーク内のダウンサンプリングとアップサンプリングのために、ストライドされた、および部分的にストライドされた畳み込みを使用します。\n",
    "\n",
    "私たちのネットワーク本体は、[44]のアーキテクチャを使って5つの残差ブロック[43]から構成されています。\n",
    "\n",
    "すべての非残留畳み込みレイヤの後に出力バイナンスを使用して出力イメージが[0、255]の範囲のピクセルを持つことを保証する出力レイヤーを除いて、\n",
    "空間バッチ正規化[45]とReLU非線形性が続きます。\n",
    "\n",
    "9×9カーネルを使用する最初と最後のレイヤーを除いて、すべての畳み込みレイヤーは3×3カーネルを使用します。\n",
    "\n",
    "すべてのネットワークの正確なアーキテクチャは、補足資料に記載されています。\n",
    "\n",
    "#### 入力と出力\n",
    "\n",
    "画風変換の場合、入力と出力は両方とも3×256×256のカラーイメージです。\n",
    "\n",
    "アップサンプリング係数がfの超解像度の場合、\n",
    "出力は3×288×288の高解像度画像パッチであり、入力は3 × 288/f × 288/fの低解像度パッチである。\n",
    "\n",
    "画像変換ネットワークは完全畳み込み型（FCN）であるため、テスト時には任意の解像度の画像に適用できます。\n",
    "\n",
    "#### ダウンサンプリングおよびアップサンプリング\n",
    "\n",
    "アップサンプリング係数がfである超解像では、いくつかの残差ブロックを使用し、続いてlog2 f畳み込みレイヤーとストライド1/2を使用します。\n",
    "\n",
    "これは、バイキュービック補間を使用してネットワークに渡す前に、低解像度入力をアップサンプリングする[1]とは異なります。\n",
    "\n",
    "固定アップサンプリング機能に頼るのではなく、分数ストライドされた畳み込みは、\n",
    "アップサンプリング機能を残りのネットワークと共同して学習することを可能にする。\n",
    "\n",
    "図3。\n",
    "\n",
    "[6]と同様に、我々は、最適化を使用して、\n",
    "予めトレーニングされたVGG-16損失ネットワークφからいくつかの層jについての特徴再構成損失フィーチャを最小にする画像yを見つける。\n",
    "より高いレイヤーから再構成すると、イメージの内容と全体的な空間構造は保持されますが、色、テクスチャ、および正確な形状は維持されません。\n",
    "\n",
    "画風変換のために、私たちのネットワークでは、2つのストライドが2である畳み込みを使用して入力をダウンサンプリングし、\n",
    "いくつかの残差ブロックを、次に2つの畳み込みレイヤーをストライド1/2からアップサンプルします。\n",
    "\n",
    "入力と出力のサイズは同じですが、ダウンサンプリングしてアップサンプリングするネットワークにはいくつかの利点があります。\n",
    "\n",
    "最初は計算式です。\n",
    "\n",
    "素朴な実装では、サイズC×H×Wの入力にCフィルタを用いた3×3畳み込みは、\n",
    "形状DCの入力にDCフィルタを用いた3×3畳み込みと同じコストである9HW C2乗算を必要とする×H / D×W / Dとなる。\n",
    "ダウンサンプリング後、同じ計算コストでより大きなネットワークを使用することができます。\n",
    "\n",
    "第2の利点は、有効な受容野サイズと関係がある。\n",
    "\n",
    "高品質のスタイル転送では、イメージの大部分を一貫して変更する必要があります。\n",
    "従って、出力における各ピクセルが入力において大きな有効受容野を有することが有利である。\n",
    "\n",
    "ダウンサンプリングを行わないと、付加的な3×3の畳み込み層がそれぞれ、有効受容野サイズを2だけ増加させる。\n",
    "\n",
    "因数Dのダウンサンプリング後、各3×3畳み込みは、代わりに、有効受容野サイズを2Dだけ増加させ、同じ数の層でより大きな有効受容野を与える。\n",
    "\n",
    "#### Residual Connections\n",
    "\n",
    "Heら[43]は、画像分類のために非常に深いネットワークを訓練するためにResidual Connectionsを使用する。\n",
    "\n",
    "彼らは、Residual Connectionsがネットワークが識別機能を学ぶことを容易にすると主張している。\n",
    "ほとんどの場合、出力画像は入力画像と構造を共有する必要があるため、\n",
    "これは画像変換ネットワークの魅力的な特性です。\n",
    "\n",
    "したがって、我々のネットワークの本体は、それぞれが2つの3×3の畳み込み層を含むいくつかの残差ブロックからなる。\n",
    "補足資料に示されている[44]の残差ブロック設計を使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3 Perceptual Loss Functions\n",
    "\n",
    "画像間の高レベル知覚および意味論的差を測定する2つの知覚損失関数を定義する。\n",
    "\n",
    "それらは、画像分類のために予め訓練された損失ネットワークφを利用し、\n",
    "これらの知覚損失関数自体が深い畳み込みニューラルネットワークであることを意味する。\n",
    "\n",
    "我々の実験では、φはImageNetデータセットで事前に計算された16層VGGネットワークである[47]。\n",
    "\n",
    "図4. [10]と同様に、\n",
    "我々は、最適化を使用して、予めトレーニングされたVGG-16損失ネットワークφからいくつかの層jについてのスタイル再構築損失を最小化する画像yを見つける。\n",
    "画像yは、文体的特徴を保存するが、空間的構造は保存しない。\n",
    "\n",
    "#### 3.1 特徴再構成損失\n",
    "\n",
    "出力画像y = fW（x）のピクセルをターゲット画像yのピクセルと正確に一致させるように促す代わりに、\n",
    "それらは損失ネットワークφによって計算される類似の特徴表現を有するように促す。\n",
    "\n",
    "画像xを処理するとき、φj（x）をネットワークφのj番目の層の活性化とする。 jが畳み込み層の場合、φj（x）は形状Cj×Hj×Wjの特徴マップとなる。\n",
    "\n",
    "特徴再構成損失は、特徴表現間の（二乗正規化された）ユークリッド距離である。\n",
    "\n",
    "[6]に示され図3に再現されているように、\n",
    "初期層の特徴再構成損失を最小にする画像yを見出すことは、視覚的にyと区別できない画像を生成する傾向がある。\n",
    "\n",
    "より高いレイヤーから再構成すると、イメージの内容と全体の空間構造は保持されますが、\n",
    "色、テクスチャ、および正確な形状は保持されません。 \n",
    "我々の画像変換ネットワークを訓練するための特徴再構成損失を使用することは、\n",
    "出力画像yが目標画像yに知覚的に類似することを助長するが、それらを正確に一致させることはない。\n",
    "\n",
    "#### 3.2 画風再構成損失\n",
    "\n",
    "特徴再構築損失は、ターゲットyからのコンテンツを逸脱するとき、出力画像yにペナルティを課す。 \n",
    "スタイルの違い、色、テクスチャ、共通パターンなどにもペナルティを課したい。\n",
    "\n",
    "この効果を達成するために、Gatysら[9,10]は、以下のスタイル再構成損失を提案する。\n",
    "\n",
    "上のように、φj（x）をネットワークφのj番目の層での活性化入力xは、形状Cj×Hj×Wjの特徴マップである。\n",
    "グラム行列Gφj（x）を、要素が以下の式で与えられるCj×Cj行列と定義する\n",
    "\n",
    "我々がφj（x）を各点のCj次元特徴を与えるものとして解釈すると\n",
    "Hj×Wjグリッドの場合、Gφj（x）は、各グリッド位置を独立したサンプルとして扱うCj次元の特徴。\n",
    "\n",
    "したがって、どの機能が一緒に作動するのかについての情報を取得します。\n",
    "\n",
    "そのグラム行列は、行列ψにφj（x）を再構成することによって効率的に計算することができる。\n",
    "形状Cj×HjWj; Gφj（x）=ΨψT/ CjHjWjとなる。\n",
    "\n",
    "スタイル再構成損失は、出力画像とターゲット画像のグラム行列の差の二乗Frobeniusノルムです。\n",
    "\n",
    "スタイル再構成の損失は、yとyのサイズが異なる場合でも、そのグラム行列は両方とも同じ形状になるため、明確に定義されます。\n",
    "\n",
    "[10]に示され図5で再現されるように、スタイル再構成損失を最小にする画像yを生成することは、\n",
    "ターゲット画像からの筆跡を保存するが、その空間的構造を保存しない。\n",
    "より高い層からの再構成は、ターゲット画像からより大きなスケールの構造を転送する。\n",
    "\n",
    "単一のレイヤーではなく、レイヤーJのセットからスタイルの再構成を実行する\n",
    "層jでは、φ、J style（y、y）を各層j∈Jの損失の総和と定義する。\n",
    "\n",
    "#### 3.3 Simple Loss Functions\n",
    "\n",
    "上で定義した知覚損失に加えて、我々はまた、低レベル画素情報にのみ依存する2つの単純な損失関数を定義する。\n",
    "\n",
    "- Pixel Loss\n",
    "\n",
    "ピクセル損失は、（正規化された）ユークリッド距離である。\n",
    "出力画像yとターゲットyとの間の距離である。 両方が形状C×H×Wを有する場合、ピクセル\n",
    "損失はピクセル（y、y）= ky-yk2 2 / CHWとして定義される。\n",
    "これは、ネットワークが一致すると予想されるチェックのためのターゲットyがある場合にのみ使用できます。\n",
    "\n",
    "- Total Variation Regularization\n",
    "\n",
    "出力画像yにおける空間的な滑らかさを促進するために、特徴反転[6,20]および超解像度[48,49]に関する先行研究に続き、全変動正則化装置T V（y）を使用する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Experiments\n",
    "\n",
    "我々は、2つの画像変換タスク、すなわち画風変換と単一画像超解像に関する実験を行う。\n",
    "画風変換に関する以前の研究では、最適化を使って画像を生成していました。 \n",
    "当社のフィードフォワードネットワークは、同様の定性的結果をもたらしますが、最大3桁高速です。\n",
    "\n",
    "畳み込みニューラルネットワークを用いた単一画像の超解像度に関する以前の研究では、ピクセルごとの損失が使用されていた。\n",
    "私たちは代わりに知覚的損失を使用することによって奨励的な定性的結果を示します。\n",
    "\n",
    "### 4.1 Style Transfer\n",
    "\n",
    "スタイル転送の目的は、ターゲットコンテンツ画像ycのコンテンツとターゲットスタイル画像ysのスタイルとを組み合わせた画像yを生成することである。\n",
    "\n",
    "私たちは、いくつかの手で選んだスタイルターゲットのためのスタイルターゲットごとに1つの画像変換ネットワークを訓練し、\n",
    "Gatysら[10]のベースラインアプローチとその結果を比較します。\n",
    "\n",
    "図5。\n",
    "\n",
    "私たちのスタイル転送ネットワークと[10]同じ目的を最小限に抑える。\n",
    "彼らの目標値を50枚の画像で比較する。 破線および誤差バーは標準偏差を示す。\n",
    "我々のネットワークは256×256の画像で訓練されるが、より大きな画像に一般化される。\n",
    "\n",
    "#### Baseline\n",
    "ベースラインとして、我々はGatysら[10]の方法を再実装する。\n",
    "与えられたスタイルとコンテンツは、実行するyとyとレイヤjとjを対象とします\n",
    "λc、λs、λTVがスカラであり、yが白色雑音で初期化され、L-BFGSを用いて最適化が行われるという問題を解くことによって画像yが生成される。\n",
    "式5の制約のない最適化は、ピクセルが範囲[0,255]の外にある画像をもたらすことが分かった。\n",
    "\n",
    "出力がこの範囲に制限されている本発明の方法とのより公平な比較のために、\n",
    "ベースラインについては、反復ごとに画像yを範囲[0,255]にクリッピングすることによって投影L-BFGSを使用して式5を最小化する。\n",
    "\n",
    "ほとんどの場合、最適化は500回の反復で満足のいく結果に収束します。\n",
    "各L-BFGS反復は、VGG-16損失ネットワークφを通る順方向および逆方向パスを必要とするため、この方法は遅い。\n",
    "\n",
    "#### Training Details\n",
    "\n",
    "私たちのスタイル転送ネットワークは、Microsoft COCOデータセット[50]で訓練されています。 \n",
    "80kのトレーニング画像のそれぞれを256×256にリサイズし、40,000回の反復で4のバッチサイズでネットワークをトレーニングし、\n",
    "トレーニングデータにおよそ2エポックを与えます。学習率1×10-3のAdam [51]を使用します。\n",
    "\n",
    "出力画像は、クロスバリデーション・スタイルのターゲットを介して選択された、1×10-6〜1×10-4の強度を有する全変動正則化で規則化される。\n",
    "\n",
    "モデルは2エポック以内に過度にフィットしないので、ウェイト減衰またはドロップアウトを使用しません。\n",
    "すべてのスタイル転送実験では、VGG-16損失ネットワークφのrelu1_2、relu2_2、relu3_3、およびrelu4_3レイヤーで、\n",
    "relu2_2レイヤーでのフィーチャー再構成損失とスタイル再構成損失を計算します。\n",
    "\n",
    "私たちの実装ではTorch [52]とcuDNN [53]を使用しています。 1つのGTX Titan X GPUで約4時間のトレーニングが必要です。\n",
    "\n",
    "#### Qualitative Results\n",
    "\n",
    "図6では、さまざまなスタイルおよびコンテンツイメージのベースライン方法の結果と当社の結果を比較する定性的な例を示します。\n",
    "\n",
    "すべての場合において、超パラメータλc、λs、およびλTVは、2つの方法の間で全く同じである。\n",
    "すべてのコンテンツ画像は、MS-COCO2014検証セットから取得される。\n",
    "全体的に我々の結果は、質的にベースラインに似ています。\n",
    "私たちのモデルは256×256の画像で訓練されていますが、テスト時にはどのようなサイズの画像にも完全畳み込み方式で適用できます。\n",
    "\n",
    "図7では、我々のモデルを用いた512×512画像のスタイル転送の例を示す。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図6。\n",
    "\n",
    "イメージ変換ネットワークを使用したスタイル転送の結果例\n",
    "我々の結果はGatysら[10]と質的に類似しているが、生成するのがはるかに高速である（表1参照）。 生成される画像はすべて256×256ピクセルです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図7。\n",
    "\n",
    "512×512画像のスタイル転送の結果の例\n",
    "このモデルは、テスト時に高解像度画像に完全畳み込み方式で適用されます。 スタイルイメージは図6と同じです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これらの結果において、訓練されたスタイル転送ネットワークが画像の意味内容を認識していることは明らかである。\n",
    "\n",
    "例えば、図7のビーチの画像では、人々は変形された画像ではっきりと認識できるが、背景は認識を超えてゆがんでいる。\n",
    "同じように猫の画像では、猫の顔は、変換された画像ではっきりしていますが、その体はそうではありません。\n",
    "\n",
    "1つの説明は、VGG-16消失ネットワークが、訓練された分類データセットに存在するので、\n",
    "人々および動物にとって選択的な特徴を有することである。\n",
    "私たちのスタイル転送ネットワークは、VGG-16の機能を維持するように訓練されています。\n",
    "そのため、背景物から人や動物を識別する方法を学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定量的結果\n",
    "\n",
    "ベースラインおよび本発明の方法は共に式5を最小化する。\n",
    "\n",
    "ベースラインは、出力画像に対して明示的な最適化を実行するが、我々の方法は、単一の順方向パスで任意のコンテンツ画像ycの解を見つけるように訓練される。\n",
    "\n",
    "したがって、式5をうまく最小にする程度を測定することによって、2つの方法を定量的に比較することができます。\n",
    "\n",
    "パブロ・ピカソによるMuseをスタイル・イメージとして使用して、MS-COCO検証セットからの50個のイメージについて、メソッドとベースラインを実行します。\n",
    "\n",
    "ベースラインについては、各最適化反復で目的関数の値を記録し、我々の方法では、各画像について式5の値を記録する。 yがコンテンツ画像ycと等しい場合、式5の値も計算します。結果を図5に示す。\n",
    "\n",
    "コンテンツ画像ycは非常に高い損失を達成し、本発明の方法は明示的最適化の50と100の反復に匹敵する損失を達成することがわかる。\n",
    "\n",
    "我々のネットワークは、256×256の画像について式5を最小限に抑えるように訓練されているが、より大きな画像に適用したときに目的を最小化するのにも成功する。\n",
    "\n",
    "512×512と1024×1024で50枚の画像について同じ定量評価を繰り返す。結果を図5に示します。高分解能であっても、このモデルではベースライン法の50〜100回の反復に匹敵する損失が達成されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表1。\n",
    "\n",
    "私たちのスタイル転送ネットワークの速度（秒）と、反復回数とイメージ解像度の変化のための最適化ベースのベースライン。\n",
    "\n",
    "私たちの方法は、同様の定性的結果をもたらしますが（図6参照）、ベースライン方法の単一の最適化ステップよりも速いです。\n",
    "\n",
    "どちらの方法もGTX Titan X GPUでベンチマークされています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 速度\n",
    "\n",
    "表1では、本発明の方法の実行時間といくつかの画像サイズのベースラインとを比較する。 \n",
    "\n",
    "ベースラインについては、さまざまな回数の最適化反復の回数を報告します。 \n",
    "\n",
    "すべての画像サイズにわたって、我々の方法の実行時間は、ベースライン方法の1回の反復の速度の約2倍であることがわかる。\n",
    "\n",
    "ベースライン方法の500反復と比較して、本発明者らの方法は3桁高速である。 \n",
    "\n",
    "この方法では、サイズが512×512の画像を20FPSで処理し、リアルタイムまたはビデオでスタイル転送を実行することが可能です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Single-Image Super-Resolution\n",
    "\n",
    "単一画像超解像度では、低解像度入力から高解像度出力画像を生成することが課題です。\n",
    "\n",
    "これは、低解像度の画像ごとに複数の高解像度画像が存在する可能性があるため、本質的に問題があります。\n",
    "\n",
    "超解像係数が大きくなるにつれて、あいまいさはより極端になります。大きな要因（×4、×8）の場合、高解像度画像の細部は、低解像度バージョンではほとんどまたはまったく証拠を持たないことがあります。\n",
    "\n",
    "この問題を克服するために、我々は、典型的に使用されるピクセルごとの損失ではなく、\n",
    "備的な損失ネットワークから超解像ネットワークへの意味論的知識の転送を可能にする特徴再構築損失（第3節参照）を訓練する。\n",
    "\n",
    "我々は、より大きな要因が入力に関してより意味論的な推論を必要とするので、×4および×8の超解像度に焦点を当てる。\n",
    "\n",
    "超解像度を評価するために使用される従来の測定基準は、PSNRとSSIMであり、両方とも人間の視覚品質の評価とはほとんど相関していません[55,56,57,58,59]。\n",
    "\n",
    "PSNRとSSIMはピクセル間の低レベルの差にのみ依存し、超解像には無効である可能性がある加法的ガウスノイズを前提として動作します。\n",
    "\n",
    "さらに、PSNRはピクセルごとの損失のピクセルと同等であるため、PSNRで測定した場合、ピクセルごとの損失を最小限に抑えるように訓練されたモデルは、再構成損失を最小限に抑えるために訓練されたモデルより常に優れているはずです。\n",
    "\n",
    "したがって、これらの実験の目的は、最先端のPSNRやSSIMの結果を達成するのではなく、ピクセル単位で再現されたモデルとフィーチャの再構成による損失を訓練したモデル間の質的な違いを示すことです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの詳細\n",
    "\n",
    "我々は、VGG-16損失ネットワークφからのrelu2_2層の特徴再構成損失を最小にすることによって、x 4およびx 8超解像を実行するモデルを訓練する。\n",
    "\n",
    "MS-COCOトレーニングセットの10k画像から288×288のパッチを訓練し、幅σ= 1.0のガウスカーネルでぼかして低解像度入力を準備し、バイキュービック補間によるダウンサンプリングを行います。\n",
    "\n",
    "Adam [51]を使用して、学習率1×10-3の200k回の反復で、バッチサイズ4でトレーニングします（ウェイト減衰またはドロップアウトなし）。\n",
    "後処理ステップとして、ネットワーク出力と低解像度入力の間のヒストグラムマッチングを実行します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ベースライン\n",
    "\n",
    "ベースラインモデルとして、SRCNN [1]を使用して最先端のパフォーマンスを実現しています。\n",
    "\n",
    "SRCNNは、ILSVRC 2013検出データセットからの33×33パッチのピクセル単位の損失を最小限に抑えるように訓練された3層の畳み込みネットワークです。\n",
    "\n",
    "SRCNNは、×8の超解像に対して訓練されていないので、×4でしか評価することができない。\n",
    "\n",
    "SRCNNは、計算上ではない109回以上の反復について訓練されている私たちのモデルには実現可能です。\n",
    "\n",
    "SRCNNと我々のモデルとのデータ、トレーニング、アーキテクチャの違いを説明するために、\n",
    "ピクセル変換を用いて4倍と8倍の超解像度の画像変換ネットワークを訓練する。\n",
    "\n",
    "これらのネットワークは、同じデータ、アーキテクチャ、およびトレーニングを使用して、ネットワークを最小限に抑えるように訓練しています。 \n",
    "\n",
    "### 評価\n",
    "\n",
    "標準のSet5 [60]、Set14 [61]、およびBSD100 [41]データセットですべてのモデルを評価します。 \n",
    "\n",
    "我々は[1,39]に続いて、YCbCr色空間に変換した後、Yチャネル上でのみ両方を計算するPSNRとSSIM [54]を報告する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果\n",
    "\n",
    "図8の×4超解像度の結果を示します。\n",
    "他の方法は、私たちのモデルは、機能の再構築のための訓練は非常に良い\n",
    "第1の画像の睫毛および第2の画像の帽子の個々の要素のような、鮮明なエッジおよび細部の細部を再構成する際の仕事である。\n",
    "\n",
    "特徴再構成損失は、倍率の下で目に見えるわずかなクロスハッチパターンを生じ、ベースライン方法と比較してそのPSNRおよびSSIMに害を与える。\n",
    "\n",
    "×8超解像度の結果を図9に示す。\n",
    "\n",
    "ここでも、私たちのモデルは、馬の足や蹄などの他のモデルに比べてエッジや細かいディテールで優れた仕事をしていることがわかります。\n",
    "\n",
    "モデルはエッジを無差別にシャープにしません。\n",
    "\n",
    "モデルと比較して、モデルは馬とライダーの境界エッジを鮮明にしますが、背景ツリーは拡散したままであり、\n",
    "\n",
    "モデルが画像セマンティクスをより認識している可能性があることを示唆しています。\n",
    "\n",
    "私たちのピクセルと私たちのモデルは同じアーキテクチャ、データ、トレーニング手順を共有しているので、\n",
    "\n",
    "それらの違いはピクセルとロスの違いによるものです。 \n",
    "\n",
    "ピクセルの損失は視覚的なアーティファクトやPSNRの値が少なくなりますが、\n",
    "細かい部分を再構成することで損失が改善され、視覚的な結果が得られます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図8。\n",
    "\n",
    "Set5（上）およびSet14（下）の画像上の×4超解像度の結果。 各例のPSNR / SSIMと、各データセットの平均値を報告します。 より多くの結果\n",
    "補足資料に示されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "図9。\n",
    "\n",
    "BSD100データセットの画像に倍率×8の超解像度結果。 \n",
    "\n",
    "サンプル画像と各データセットの平均について、PSNR / SSIMを報告します。 補足資料に、より多くの結果が示されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Conclusion\n",
    "\n",
    "本稿では、フィードフォワード変換ネットワークと知覚損失関数を訓練することによって、\n",
    "フィードフォワード画像変換タスクと最適化に基づく画像生成手法の利点を組み合わせています。\n",
    "\n",
    "私たちは、既存の手法に匹敵する性能を達成し、既存の方法と比較して大幅にスピードを上げたスタイル転送と\n",
    "知覚損失を伴う訓練が細部とエッジを良好に再現できることを示す単一画像の超解像度化へこの手法を適用した。\n",
    "\n",
    "今後の研究では、色彩化やセマンティックセグメンテーションなど、他の画像変換タスクに知覚損失関数を使用することを検討したいと考えています。\n",
    "\n",
    "また、さまざまな損失ネットワークを使用して、\n",
    "異なるタスクやデータセットで訓練された損失ネットワークがさまざまな種類の意味論的知識を\n",
    "画像変換ネットワークに付与できるかどうかを調べる予定です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
